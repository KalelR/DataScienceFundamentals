{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2b4b6a-5c26-4834-b986-11e512c98269",
   "metadata": {},
   "source": [
    "# 3.3 - How reliable are the model's parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df330973-0b78-46e0-b04c-a59ca98e17e9",
   "metadata": {},
   "source": [
    "In 3.2 we measure how much the model's predictions deviate from the real data. Now we focus on the parameters themselves, and consider how much we can trust that they are correct. For example, when applying linear regression to a dataset of sales against advertising, we may get a slope parameter of 0.1. In this case, how are we sure that there is indeed a positive relation between advertising and sales? How can we guarantee that the real slope isn't -0.05, and we got 0.1 just because of noise?\n",
    "\n",
    "There are a few ways to approach this.\n",
    "\n",
    "## Bootstrapping\n",
    "Imagine we repeat a measurement on the same features a number of times. The response variable we measure will have some dispersion, induced by the noise in measurements (at the very least). If we model a dataset with a single realization per feature, we will have access to only one of those trials, which are all dispersed. As a consequence, the coefficients we get will not be the true coefficients, and each realization will lead to models with different coefficients. If we have access to several realizations, we can then see how much they disperse. If we assume the noise is unbiased, we can also get an idea of where the true value (at the center) is.  \n",
    "\n",
    "![](images/dispersion-different-realizations.png)\n",
    "\n",
    "This is super useful, but of course requires several different realizations. If we get only a single realization (i.e., a single dataset), the idea of _boostrapping_ allows us to effectively create different synthetic realizations, thereby *allowing us to estimate errors and distributions of the coefficients from a single realization*.\n",
    "\n",
    "If the dataset has $N$ observations, we can randomly select a subset of size $N_c < N$ with potential duplicates. Each sample is treated as a different realization.\n",
    "\n",
    "## Feature importance graph\n",
    "From the bootstrapped method, one can compute the mean and standard deviation over the coefficients. An easy way to visualize this is to then plot a feature importance graph. For each feature, get the corresponding coefficient's mean and std values and plot them on the x-axis. \n",
    "\n",
    "![](images/feature-importance-graph.png)\n",
    "\n",
    "## Confidence intervals\n",
    "\n",
    "A confidence interval of a response variable is an interval in which there is a fixed change (e.g., 95%) that the true response variable intervals will be inside. For example, for a 95% confidence interval, there is a 95% chance that the true value is inside. \n",
    "\n",
    "This is estimated from the parameters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a66c12-d7dc-454e-b2c3-66f5e5378e67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4034da-e278-46dc-aacc-8d52dc983718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247f984-f4d3-445d-92e8-5524f340f45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
