{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb19a7-c20e-47a3-a613-07ff4ff31330",
   "metadata": {},
   "source": [
    "# 1.4 - Regularized Linear Regression - LASSO and RIDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf84e19-081b-4334-85b0-88ad9f7b12e8",
   "metadata": {},
   "source": [
    "__Input__: Dataset $((x_{11}, x_{12}, \\cdots, x_{1n}, y_1), (x_{21}, x_{22}, \\cdots, x_{2n}, y_2), \\cdots, (x_{N1}, x_{N2}, \\cdots, x_{Nn}, y_N))$ with __multiple__ (n) features and a response variable, with $P$ measurements\n",
    "\n",
    "__Purpose__: Find the \"best\" linear function $f(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n$ that fits the data, **trying to correct for potential overfitting**. This can improve the generality and interpretability of the model. Can also mitigate problems with collinearity, which happen more often when more features are present.\n",
    "\n",
    "__Approach__: Add a cost term to the loss function which we are optimizing for in linear regression. Equivalently, add a constraint to the optimization problem. The lasso method tries to turn as many coefficients into zero.\n",
    "\n",
    "__Assumptions__: The lasso method assumes that the coefficients of the linear model are sparse, meaning that few of them are non-zero. \n",
    "\n",
    "__Hyperparameters__: \n",
    "\n",
    "In some circumstances, e.g. when the data spans different orders of magnitude, simple linear regression can yield widely varying coefficients, spanning orders of magnitude. This may not represent the general behavior of the system, only of the dataset itself the model was trained on. In this case, the model is said to be overfitted (see more in Sec. 3.X).\n",
    "\n",
    "Simple linear regression is attempting to minimize the MSE, $\\mathrm{MSE} = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 = \\frac{1}{N}\\sum_{i=1}^{n} (y_i - \\beta X)^2$ (for $\\beta$ and $X$ defined as in the multilinear regression of Sec. 1.2). \n",
    "\n",
    "The LASSO (least absolute shrinkage and selection operator) adds the constraint that $\\sum_{j=1}^n |\\beta_j| \\leq \\lambda$. Or equivalently one adds a loss term to the cost function (the MSE), such that it becomes $L_{LASSO} = \\frac{1}{n} \\sum_{i=1}^N ||\\hat{Y} - X_i^T \\hat{\\beta}||^2 + \\lambda \\sum_{j=1}^n |\\beta_j|$. \n",
    "\n",
    "The RIDGE method is a similar approach, but adds the constraint in l2 norm: $\\sum_{j=1}^n \\beta_j^2 \\leq \\lambda$.\n",
    "\n",
    "If $\\lambda = 0$, there is no extra cost, if $\\lambda \\to \\infty$, the cost is too high and all coefficients go to zero. Somewhere in the middle there should be an optimal point.\n",
    "\n",
    "The LASSO method has no closed analytical form, so techniques like gradient descent must be used, making it slower than RIDGE. But it is nice because it performs **variable seletion**: often some of the coefficients become 0, leaving only a smaller subset of \"more important\" features, which makes the model easier to interpret.\n",
    "\n",
    "The RIDGE method has a closed form, so it is faster. But it does not necessarily perform variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917e4b8-df27-4b4d-8f0e-4f7802096746",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-a-gradient-descent-algorithm-with-regularization-from-scratch-in-python-571cb1b46642\n",
    "https://machinelearningcompass.com/machine_learning_models/lasso_regression/\n",
    "https://machinelearningcompass.com/machine_learning_math/gradient_descent_for_linear_regression/#the-gradient\n",
    "http://www.cs.cmu.edu/afs/cs/project/link-3/lafferty/www/ml-stat2/talks/YondaiKimGLasso-SLIDE-YD.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b6254-3946-4c34-9410-f65ab9b38cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12757cbb-f9e4-41c6-ae77-d89e3c13d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
